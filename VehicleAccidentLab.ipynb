{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Experience Hands On Lab1 - Python\n",
    "\n",
    "The first thing we need to do is connect to a data source and read this into a spark sql dataframe.  Dataframes were new constructs starting in Spark 1.6 and provide a much richer symantex than the prior RDD approach.  \n",
    "\n",
    "### Step 1 - Click on the \"Find and Add Data\" icon upper right (series of 1 & 0s) \n",
    "\n",
    "### Step 2 - In right panel under files, select \"browse\" and select the ACCIDENT2007-FullDataSet.csv file\n",
    "\n",
    "Your File is locaed in project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will insert code into your notebook for that data file you loaded, so make sure your cusrson is in the next code cell below.\n",
    "\n",
    "### Step 3 - Under file name, pick \"Insert to code\" > \"Insert Spark SQL DataFrame\" \n",
    "\n",
    "This will create a connection to the spark object store where the file is stored and create a resulting Spark SQL DataFrame for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Put your Cursor in this cell\n",
    "### Do step 3 above\n",
    "### Once code is pasted in here, Run the code in the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a dataframe object, you can do analysis based on this such as performing correlation analysis on various variables\n",
    "\n",
    "You can see the correlation between the individual was drunk and if the accident resulted in fatalities by performing a simple pearson correlation on the two variables.  The notebooks provide code assistance by using the tab after a dot notation to see the various functions availabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run this code\n",
    "df_data_1.corr('DRUNK_DR','FATALS','pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSX also provides the ability to import your favorite libraries to use within your notebooks, for example Pandas.  \n",
    "\n",
    "Import the pandas library and use it to convert the Spark DataFrame to a Pandas DataFrame and use the head function to show the top 5 rows values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run this code\n",
    "import pandas as pd\n",
    "pd_fars = df_data_1.toPandas()\n",
    "pd_fars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get summary statistics using the describe function.  This can help you determine missing values and the distribution of your attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run this code\n",
    "pd_fars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to look at an individual states worth of data.  The Spark DataFrame object supports a filter option to allow you to filter the data based on a column of interest and the resulting value.  \n",
    "\n",
    "To see a list of the states, the FARS data dictionairy provides a reference to the state codes here - https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812092 .  Choose your state of interest to filter the list down to your particular state.  Convert the results to a new Panda dataframe and view the first 5 rows data.  Below is a list of some of the state values;\n",
    "\n",
    "        12 - Florida      11 - DC          13 - Georgia         48 - Texas    39 - Ohio     36 - New York        34 - New Jersey\n",
    "        53 - Washingon    51 - Virginia    37 - North Carolina  40 - Oklahoma 32 - Neveda   45 - South Carolina  04 - Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run this code\n",
    "df_cal=df_data_1.filter(df_data_1['STATE']==6)\n",
    "pd_cal=df_cal.toPandas()\n",
    "pd_cal.head(5)\n",
    "pd_cal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We would now like to map out the occurrences for the state of interest but if you look at the LATITUDE and LONGITUD values, these have not been declared as a float.\n",
    "\n",
    "We can create new columns for a modified version of these fields so that we can map the individual occurances on a map.\n",
    "\n",
    "Use lamba functions to create a lon and lat column that represents the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run this codep\n",
    "d_cal['lat'] = pd_cal['LATITUDE'].map(lambda x: (x * 1.0) / 1000000)\n",
    "pd_cal['lon'] = pd_cal['LONGITUD'].map(lambda x: (x * -1.0) / 1000000)\n",
    "pd_cal.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we need, we can import the brunel package and use it to show a graphical map of the occurances\n",
    "\n",
    "Use the following code to display the map for your state using the lon and lat values and use PERSONS to display a color scale based on the number of individuals involved in the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run this code\n",
    "import brunel\n",
    "\n",
    "%brunel map ('CA') + data('pd_cal') x(lon) y(lat) color(PERSONS) tooltip(VE_TOTAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another graphics package that was created by IBM is the pixie dust package.  Pixie dust provides an easy way to visualize the data using various \n",
    "charts.  \n",
    "\n",
    "To import the pixiedust package, you simply need to use and import statement (from pixiedust.display import *)\n",
    "\n",
    "Once imported, you bring up the interactive display area by using the display function on your dataset - for example display(df_data_1).\n",
    "\n",
    "The initial display is a table view of the dataframe.  \n",
    "\n",
    "Switch views to the pie chart by selecting the middle charting drop down menu at the top left of the display area. This will display a pie chart of the count of accidents by state along with a percentage.  You can view and modify the options used for the display by selecting the paint brush icon at the bottom left of the display area (note this may be invisible until you hover near the area with your mouse).  If you change the value to city instead of state, you will see a busier graph.\n",
    "\n",
    "You can switch to a different dataframe at anytime by changing the value in the display parameter and rerunning the cell.  \n",
    "\n",
    "Change the display to use the california data display(df_cal).\n",
    "\n",
    "select the histogram chart from the drop down and then modify the values to contain city.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "handlerId": "histogram",
      "keyFields": "STATE",
      "staticFigure": "false",
      "valueFields": "STATE"
     }
    }
   },
   "outputs": [],
   "source": [
    "### Run this code\n",
    "from pixiedust.display import *\n",
    "display(df_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "At this point we are ready to start our first pass at building a predictive model.  In this example we will use the statsmodel.formula.api package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run this code\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "result = sm.ols(formula=\"FATALS ~ VE_TOTAL + PERSONS + WEATHER + VE_FORMS\", data=pd_cal).fit()\n",
    "print result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a summary of all the results, use print result.summary().\n",
    "\n",
    "As you can see from the results, this is not a good model at all.  Select a different set of attributes to see if you can improve the R-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run this code\n",
    "print result.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}